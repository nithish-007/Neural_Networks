{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      " [[ 1.6706  -2.4841  -3.4835 ]\n",
      " [ 0.7634  -3.0764  -5.8985 ]\n",
      " [-1.38314  1.66994 -0.25515]] \n",
      " Output layer dim: (3, 3)\n"
     ]
    }
   ],
   "source": [
    "inputs = [[1, 2, 3, 4],\n",
    "          [2, 5, -1, 2],\n",
    "          [-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "# layer 1 \" single neuron\"\n",
    "weights = [[0.2, 0.8, -0.5, 1.0],\n",
    "           [0.5, -0.91, 0.26, -0.5],\n",
    "           [0.26, -0.27, 0.17, 0.87]]\n",
    "biases = [2, 3, 0.5]\n",
    "\"layer 1 output dim 3x3\"\n",
    "\n",
    "# layer 2\n",
    "weights2 = [[0.1, -0.14, 0.5],\n",
    "           [-0.5, 0.12, -0.33],\n",
    "           [-0.44, 0.73, -0.13]]\n",
    "biases2 = [-1, 2, -0.5]\n",
    "\"layer 2 o/p dim 3x3\"\n",
    "\n",
    "\n",
    "layer1_outputs = np.dot(inputs, np.array(weights).T) + biases\n",
    "layer2_outputs = np.dot(layer1_outputs, np.array(weights2).T) + biases2\n",
    "# input shape is broadcasted by duplicated the row \n",
    "print(\"Output:\\n\",layer2_outputs,\"\\n\", \"Output layer dim:\", layer2_outputs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output of layer1:\n",
      " [[-0.3610963  -0.24283524  0.41654341 -0.8787856  -0.99730016]\n",
      " [-0.08384138  0.6059603   0.55190831  0.07959199  0.11448039]\n",
      " [-0.24566894  0.37446278  0.16476186 -0.91395312 -0.27462437]]\n",
      "output of layer 2: \n",
      " [[ 0.0838096   0.04344583]\n",
      " [-0.05427832 -0.0786683 ]\n",
      " [ 0.07924331 -0.07230373]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "X = [[1, 2, 3, 4],\n",
    "    [2, 5, -1, 2],\n",
    "    [-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "class layer_Dense:\n",
    "    def __init__(self, n_inputs = None, n_neuron = None):\n",
    "        self.weights = 0.10 * np.random.randn(n_inputs, n_neuron) # gaussian distro\n",
    "        # by initializing weights as i/p, n_neuron avoids the transpose in the forward pass\n",
    "\n",
    "        self.biases = np.zeros((1, n_neuron))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "layer1 = layer_Dense(n_inputs= 4, n_neuron = 5)\n",
    "layer2 = layer_Dense(n_inputs= 5, n_neuron= 2)\n",
    "\n",
    "layer1.forward(X)\n",
    "print(\"output of layer1:\\n\",layer1.output)\n",
    "\n",
    "layer2.forward(layer1.output)\n",
    "print(\"output of layer 2: \\n\", layer2.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hidden Layer Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
