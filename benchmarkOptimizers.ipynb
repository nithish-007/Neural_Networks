{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data manipulation and visulization\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((64,64)),\n",
    "        transforms.RandomHorizontalFlip(p = 0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,)),\n",
    "        \n",
    "    ]\n",
    ")\n",
    "\n",
    "test_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomHorizontalFlip(p = 0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "        \n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "trainset = torchvision.datasets.FashionMNIST(root = \"./\",\n",
    "                                             train = True,\n",
    "                                             download = False,\n",
    "                                             transform= train_transform)\n",
    "testset = torchvision.datasets.FashionMNIST(root= \"./\",\n",
    "                                            train = False, \n",
    "                                            download = False,\n",
    "                                            transform= test_transform)\n",
    "\n",
    "train_dataloader = DataLoader(dataset= trainset,\n",
    "                              batch_size= 32,\n",
    "                              num_workers= 0,\n",
    "                              shuffle= True)\n",
    "\n",
    "test_dataloader = DataLoader(dataset= testset,\n",
    "                             batch_size= 32,\n",
    "                             num_workers= 0,\n",
    "                             shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 64])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TinyVGG(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super(TinyVGG, self).__init__()\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape,\n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=3,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units,\n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=3,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # Reduces height and width by 2\n",
    "        )\n",
    "\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=hidden_units,\n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=3,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units,\n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=3,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # Reduces height and width by 2\n",
    "        )\n",
    "\n",
    "        # Calculate the size of the feature map after the convolutional layers\n",
    "        self.feature_map_size = hidden_units * 7 * 7  # 7x7 after two MaxPool2d layers with stride 2\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=self.feature_map_size, out_features=output_shape)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block_1(x)\n",
    "        x = self.conv_block_2(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG-16 Summary: \n",
      " =================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "TinyVGG                                  --\n",
      "├─Sequential: 1-1                        --\n",
      "│    └─Conv2d: 2-1                       640\n",
      "│    └─ReLU: 2-2                         --\n",
      "│    └─Conv2d: 2-3                       36,928\n",
      "│    └─ReLU: 2-4                         --\n",
      "│    └─MaxPool2d: 2-5                    --\n",
      "├─Sequential: 1-2                        --\n",
      "│    └─Conv2d: 2-6                       36,928\n",
      "│    └─ReLU: 2-7                         --\n",
      "│    └─Conv2d: 2-8                       36,928\n",
      "│    └─ReLU: 2-9                         --\n",
      "│    └─MaxPool2d: 2-10                   --\n",
      "├─Sequential: 1-3                        --\n",
      "│    └─Flatten: 2-11                     --\n",
      "│    └─Linear: 2-12                      31,370\n",
      "=================================================================\n",
      "Total params: 142,794\n",
      "Trainable params: 142,794\n",
      "Non-trainable params: 0\n",
      "================================================================= \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = TinyVGG(input_shape = 1,\n",
    "              hidden_units = 64, \n",
    "              output_shape = len(trainset.classes)).to(device)\n",
    "\n",
    "from torchinfo import summary\n",
    "print(\"VGG-16 Summary:\",\"\\n\",summary(model),\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module,\n",
    "               dataloader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer):\n",
    "    \n",
    "    model.train()\n",
    "    train_loss, train_acc = 0, 0\n",
    "\n",
    "    for batch, (x,y) in enumerate(dataloader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        y_pred = model(x)\n",
    "\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim =1), dim = 1)\n",
    "        train_acc += (y_pred_class == y).sum().item() / len(y_pred_class)\n",
    "\n",
    "\n",
    "    # avg train loss and acc\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def test_step(model: torch.nn.Module,\n",
    "            dataloader: torch.utils.data.DataLoader,\n",
    "            loss_fn: torch.nn.Module):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    test_loss, test_acc = 0,0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for batch, (x,y) in enumerate(dataloader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            test_y_pred = model(x)\n",
    "\n",
    "            loss = loss_fn(test_y_pred, y)\n",
    "            test_loss += loss\n",
    "\n",
    "            test_y_pred_class = torch.argmax(torch.softmax(test_y_pred, dim = 1), dim = 1)\n",
    "            test_acc += (test_y_pred_class == y).sum().item() / len(test_y_pred_class)\n",
    "\n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    test_acc = test_acc / len(dataloader)\n",
    "\n",
    "    return test_loss, test_acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm \n",
    "# start  the timer \n",
    "from timeit import default_timer as timer \n",
    "\n",
    "def train(model:nn.Module,\n",
    "          train_dataloader: torch.utils.data.DataLoader,\n",
    "          test_dataloader: torch.utils.data.DataLoader,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module,\n",
    "          epochs: int = 50):\n",
    "    \n",
    "    # create an empty results dir\n",
    "    results = {\"train_loss\":[],\n",
    "               \"test_loss\":[],\n",
    "               \"train_acc\":[],\n",
    "               \"test_acc\":[]}\n",
    "    \n",
    "    # loop for training an testing\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train_step(model = model,\n",
    "                                           dataloader = train_dataloader,\n",
    "                                           loss_fn = loss_fn,\n",
    "                                           optimizer = optimizer)\n",
    "\n",
    "        test_loss, test_acc = test_step(model = model,\n",
    "                                        dataloader = test_dataloader,\n",
    "                                        loss_fn = loss_fn)\n",
    "\n",
    "        \n",
    "        print(\n",
    "            f\"epochs: {epoch+1} | train_loss: {train_loss} | train_acc: {train_acc} | test_loss: {test_loss} | test_acc: {test_acc}\"\n",
    "            )\n",
    "        \n",
    "        # update the results dictionary\n",
    "        results[\"train_loss\"].append(train_loss.item() if isinstance(train_loss, torch.Tensor) else train_loss)\n",
    "        results[\"train_acc\"].append(train_acc.item() if isinstance(train_acc, torch.Tensor) else train_acc)\n",
    "        results[\"test_loss\"].append(test_loss.item() if isinstance(test_loss, torch.Tensor) else test_loss)\n",
    "        results[\"test_acc\"].append(test_acc.item() if isinstance(test_acc, torch.Tensor) else test_acc)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with SGD...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x16384 and 3136x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[92], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m opt_name, opt_func \u001b[38;5;129;01min\u001b[39;00m optimizer\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mopt_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m     model_reults \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m                        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mopt_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     opt_results[opt_name] \u001b[38;5;241m=\u001b[39m model_reults\n\u001b[0;32m     30\u001b[0m end_time \u001b[38;5;241m=\u001b[39m timer()\n",
      "Cell \u001b[1;32mIn[91], line 20\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_dataloader, test_dataloader, optimizer, loss_fn, epochs)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# loop for training an testing\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m---> 20\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     test_loss, test_acc \u001b[38;5;241m=\u001b[39m test_step(model \u001b[38;5;241m=\u001b[39m model,\n\u001b[0;32m     26\u001b[0m                                     dataloader \u001b[38;5;241m=\u001b[39m test_dataloader,\n\u001b[0;32m     27\u001b[0m                                     loss_fn \u001b[38;5;241m=\u001b[39m loss_fn)\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m     31\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | train_loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | train_acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | test_loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | test_acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     32\u001b[0m         )\n",
      "Cell \u001b[1;32mIn[90], line 12\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(model, dataloader, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch, (x,y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[0;32m     10\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 12\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred, y)\n\u001b[0;32m     15\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[88], line 46\u001b[0m, in \u001b[0;36mTinyVGG.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     44\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_block_1(x)\n\u001b[0;32m     45\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_block_2(x)\n\u001b[1;32m---> 46\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\torch\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\torch\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x16384 and 3136x10)"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(params = model.parameters(), lr = 0.001)\n",
    "optimizer = {\n",
    "    \"SGD\": lambda params: torch.optim.SGD(params, lr = 0.001),\n",
    "    \"RMSprop\": lambda params: torch.optim.RMSprop(params, lr = 0.001),\n",
    "    \"Adam\": lambda params: torch.optim.Adam(params, lr = 0.001),\n",
    "    \"Adagrad\": lambda params: torch.optim.Adagrad(params, lr = 0.001),\n",
    "    \"AdaDelta\": lambda params: torch.optim.Adadelta(params, lr = 0.001)\n",
    "}\n",
    "\n",
    "start_time = timer()\n",
    "\n",
    "opt_results = {}\n",
    "for opt_name, opt_func in optimizer.items():\n",
    "    print(f\"Training with {opt_name}...\")\n",
    "\n",
    "    model_reults = train(model = model,\n",
    "                        train_dataloader= train_dataloader,\n",
    "                        test_dataloader= test_dataloader,\n",
    "                        loss_fn= loss_fn,\n",
    "                        optimizer= opt_func(model.parameters()),\n",
    "                        epochs= num_epochs)\n",
    "    opt_results[opt_name] = model_reults\n",
    "\n",
    "end_time = timer()\n",
    "\n",
    "print(f\"total training time: {end_time - start_time:.3f} seconds\")\n",
    "\n",
    "# Step 5: Visualize Results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot training loss\n",
    "plt.subplot(1, 2, 1)\n",
    "for opt_name, result in opt_results.items():\n",
    "    plt.plot(result['train_loss'], label=f\"{opt_name} Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot test accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "for opt_name, result in opt_results.items():\n",
    "    plt.plot(result['test_accuracy'], label=f\"{opt_name} Accuracy\")\n",
    "plt.title(\"Test Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Print runtimes\n",
    "for opt_name, result in opt_results.items():\n",
    "    print(f\"{opt_name} Runtime: {result['runtime']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Input z must be 2D, not 1D",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m Z \u001b[38;5;241m=\u001b[39m loss_matrix\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Plot the contour map\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m contour \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontourf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mZ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mviridis\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m plt\u001b[38;5;241m.\u001b[39mcolorbar(contour)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Add labels\u001b[39;00m\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\torch\\Lib\\site-packages\\matplotlib\\pyplot.py:3154\u001b[0m, in \u001b[0;36mcontourf\u001b[1;34m(data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3152\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mcontourf)\n\u001b[0;32m   3153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontourf\u001b[39m(\u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m QuadContourSet:\n\u001b[1;32m-> 3154\u001b[0m     __ret \u001b[38;5;241m=\u001b[39m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontourf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3155\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m   3156\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3157\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m __ret\u001b[38;5;241m.\u001b[39m_A \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   3158\u001b[0m         sci(__ret)\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\torch\\Lib\\site-packages\\matplotlib\\__init__.py:1473\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1470\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m   1471\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1472\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1473\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1474\u001b[0m \u001b[43m            \u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1475\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1476\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1478\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1479\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[0;32m   1480\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\torch\\Lib\\site-packages\\matplotlib\\axes\\_axes.py:6675\u001b[0m, in \u001b[0;36mAxes.contourf\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   6666\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   6667\u001b[0m \u001b[38;5;124;03mPlot filled contours.\u001b[39;00m\n\u001b[0;32m   6668\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   6672\u001b[0m \u001b[38;5;124;03m%(contour_doc)s\u001b[39;00m\n\u001b[0;32m   6673\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   6674\u001b[0m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilled\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m-> 6675\u001b[0m contours \u001b[38;5;241m=\u001b[39m \u001b[43mmcontour\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mQuadContourSet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6676\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request_autoscale_view()\n\u001b[0;32m   6677\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m contours\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\torch\\Lib\\site-packages\\matplotlib\\contour.py:813\u001b[0m, in \u001b[0;36mContourSet.__init__\u001b[1;34m(self, ax, levels, filled, linewidths, linestyles, hatches, alpha, origin, extent, cmap, colors, norm, vmin, vmax, extend, antialiased, nchunk, locator, transform, negative_linestyles, clip_path, *args, **kwargs)\u001b[0m\n\u001b[0;32m    809\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnegative_linestyles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    810\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnegative_linestyles \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m    811\u001b[0m         mpl\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontour.negative_linestyle\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m--> 813\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    814\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_levels()\n\u001b[0;32m    816\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extend_min \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextend \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboth\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\torch\\Lib\\site-packages\\matplotlib\\contour.py:1470\u001b[0m, in \u001b[0;36mQuadContourSet._process_args\u001b[1;34m(self, corner_mask, algorithm, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m         corner_mask \u001b[38;5;241m=\u001b[39m mpl\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontour.corner_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_corner_mask \u001b[38;5;241m=\u001b[39m corner_mask\n\u001b[1;32m-> 1470\u001b[0m x, y, z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_contour_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1472\u001b[0m contour_generator \u001b[38;5;241m=\u001b[39m contourpy\u001b[38;5;241m.\u001b[39mcontour_generator(\n\u001b[0;32m   1473\u001b[0m     x, y, z, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_algorithm, corner_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_corner_mask,\n\u001b[0;32m   1474\u001b[0m     line_type\u001b[38;5;241m=\u001b[39mcontourpy\u001b[38;5;241m.\u001b[39mLineType\u001b[38;5;241m.\u001b[39mSeparateCode,\n\u001b[0;32m   1475\u001b[0m     fill_type\u001b[38;5;241m=\u001b[39mcontourpy\u001b[38;5;241m.\u001b[39mFillType\u001b[38;5;241m.\u001b[39mOuterCode,\n\u001b[0;32m   1476\u001b[0m     chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnchunk)\n\u001b[0;32m   1478\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_transform()\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\torch\\Lib\\site-packages\\matplotlib\\contour.py:1510\u001b[0m, in \u001b[0;36mQuadContourSet._contour_args\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1508\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m<\u001b[39m nargs \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[0;32m   1509\u001b[0m     x, y, z_orig, \u001b[38;5;241m*\u001b[39margs \u001b[38;5;241m=\u001b[39m args\n\u001b[1;32m-> 1510\u001b[0m     x, y, z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_xyz\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz_orig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1512\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _api\u001b[38;5;241m.\u001b[39mnargs_error(fn, takes\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom 1 to 4\u001b[39m\u001b[38;5;124m\"\u001b[39m, given\u001b[38;5;241m=\u001b[39mnargs)\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\torch\\Lib\\site-packages\\matplotlib\\contour.py:1536\u001b[0m, in \u001b[0;36mQuadContourSet._check_xyz\u001b[1;34m(self, x, y, z, kwargs)\u001b[0m\n\u001b[0;32m   1533\u001b[0m z \u001b[38;5;241m=\u001b[39m ma\u001b[38;5;241m.\u001b[39masarray(z)\n\u001b[0;32m   1535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m z\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m-> 1536\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput z must be 2D, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mz\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m z\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m z\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m   1538\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput z must be at least a (2, 2) shaped array, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1539\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut has shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mz\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Input z must be 2D, not 1D"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "# Prepare data for contour plot\n",
    "optimizers = list(opt_results.keys())\n",
    "epochs = list(range(1, num_epochs + 1))\n",
    "# Create a 2D array of losses\n",
    "loss_matrix = np.array([opt_results[opt_name]['train_loss'] for opt_name in optimizers])\n",
    "\n",
    "# Create a contour plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "X, Y = np.meshgrid(epochs, range(len(optimizers)))  # Grid for epochs and optimizers\n",
    "Z = loss_matrix\n",
    "\n",
    "# Plot the contour map\n",
    "contour = plt.contourf(X, Y, Z, cmap='viridis', levels=20)\n",
    "plt.colorbar(contour)\n",
    "\n",
    "# Add labels\n",
    "plt.title(\"Training Loss Contour Map\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Optimizer\")\n",
    "plt.yticks(range(len(optimizers)), optimizers)\n",
    "plt.xticks(epochs)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
